---
title: Projects
layout: page
excerpt: SEA Lab -- Projects.
permalink: /projects/
---

<div class="container">
  <h1>Research</h1>
  <p>
    Our research mainly involves understanding and generating human affective and social behaviors, such as emotions, empathy and rapport. Such 
    technologies can be used to animate virtual humans or social robots. We aim to work towards technologies that do not replace but augment
    human-human interaction. We are also interested in studying the verbal and nonverbal behaviors associated with mental health disorders and
    identify the successful strategies for psychotherapy through computational approaches.
  </p>

  <h2>Behavior Generation</h2>
  <p>
    Generative modeling (Diffusion Models, GANs, etc.) of human social behaviors has broad applications 
    aiming to replicate the complex dynamics of human interactions. Our lab's recent work in this field 
    leverages generative models to simulate human nonverbal behaviors and body movements. By employing a
    blend of generative models, e.g. Diffusion Model, VQ-GAN and etc, our research strives to capture the 
    essence of how humans respond to and influence each other in social contexts. These models not only
    achieve unprecedented levels of lifelikeness and responsiveness in virtual agents but also open new
    avenues for applications in virtual reality, interactive media, and assistive technologies. By grounding these
    generative models in human-human interaction, we aim to bridge the gap between artificial representations
    and the authentic social behaviors, paving the way for artificial social intelligence.
  </p>
  <div class="row">
    <div class="col-md-3">
      <img src="/assets/images/proj_1.jpg" alt="Image 1" class="img-fluid">
    </div>
  </div>

  <h2>Representative publications</h2>
  <ul>
    <li>
      M. Tran, D. Chang, M. Siniukov, M. Soleymani. DIM: Dyadic Interaction Modeling for Social Behavior 
      Generation. European Computer Vision Conference (ECCV), Milan, Italy, 2024.
    </li>
    <li>
      D. Chang, Y. Shi, Q. Gao, H. Xu, J. Fu, G. Song, Q. Yan, Y. Zhu, X. Yang, M. Soleymani. MagicPose: Realistic Human Poses and Facial Expressions
      Retargeting with Identity-aware Diffusion. International Conference on Machine Learning (ICML), Vienna, Austria, 2024.
    </li>
  </ul>

  <h2>Emotion recognition</h2>
  <p>
    Emotions play a central role in shaping our behavior and guiding our decisions. Affective Computing
    strives to enable machines to recognize, understand and express emotions. Advancing automatic
    understanding of human emotions involves technical challenges in signal processing, computer
    vision and machine learning. The major technical challenges in emotion recognition include the lack
    of reliable labels, human variability and scarcity of labelled data. We have studied and evaluated
    machine-based emotion recognition methods from EEG signals, facial expression, pupil dilation and
    psychophysiological signals. Our past work demonstrated how behaviors associated cognitive
    state influence the perception of emotions.
  </p>
  <div class="row">
    <div class="col-md-6">
      <img src="/assets/images/proj_1.jpg" alt="Image 2" class="img-fluid">
    </div>
  </div>
</div>

